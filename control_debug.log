
=== Session Start @ 2025-07-08 22:23:20.820672 ===
[CONTROL 2025-07-08 22:23:21.111] Execute: /home/tbj/.pyenv/versions/3.10.12/bin/python -c 'import torch; print(torch.__version__)'
[CONTROL 2025-07-08 22:23:23.958] Execute: mkdir -p /home/tbj/dml_code
[CONTROL 2025-07-08 22:23:24.009] [OK] server1 connected (PyTorch 2.7.1+cu126)
[CONTROL 2025-07-08 22:23:24.388] Execute: /usr/bin/python3.10 -c 'import torch; print(torch.__version__)'
[CONTROL 2025-07-08 22:23:26.418] Execute: mkdir -p /home/tbj/dml_code
[CONTROL 2025-07-08 22:23:26.468] [OK] server2 connected (PyTorch 2.7.1+cu126)
[CONTROL 2025-07-08 22:23:26.469] Starting distributed training
[CONTROL 2025-07-08 22:23:26.469] Execute: export RANK=0 WORLD_SIZE=2 MASTER_ADDR=192.168.104.242 MASTER_PORT=29500 && cd /home/tbj/dml_code && nohup /home/tbj/.pyenv/versions/3.10.12/bin/python -m torch.distributed.run --nproc_per_node=1 --nnodes=2 --rdzv_id=resnet18_train --rdzv_backend=c10d --rdzv_endpoint=192.168.104.242:29500 /home/tbj/dml_code/dml1.py >> /home/tbj/dml_code/dml1_log 2>&1 & echo $! > /tmp/resnet_train.pid
[CONTROL 2025-07-08 22:23:41.490] [CRITICAL ERROR] 
[CONTROL 2025-07-08 22:23:41.491] Cleaning up resources
